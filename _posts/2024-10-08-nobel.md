---
title: '2024 Nobel Prize in Physics'
date: 2024-10-08
permalink: /posts/2024/10/blog-post-1/
tags:
  - physics
---
I’m honestly so hyped about how much attention spin glass theory has been getting lately. It really feels like a golden age for the field!

Just look at this wild lineup over the past few years:

2021: [Giorgio Parisi won the Nobel Prize in Physics](https://www.nobelprize.org/prizes/physics/2021/parisi/facts/) for his groundbreaking work on disordered systems—especially spin glasses—and for coming up with replica symmetry breaking. That idea completely changed the way we think about randomness in complex systems.

Then: [Michel Talagrand got the Abel Prize](https://abelprize.no/article/2024/michel-talagrand-awarded-2024-abel-prize). He took all the messy but beautiful physics behind spin glasses and turned it into solid mathematics using probability theory. He literally proved the Parisi formula—something that used to be just a theoretical physics idea!

And in 2024: [John Hopfield received the Nobel Prize in Physics](https://www.nobelprize.org/prizes/physics/2024/summary/). He introduced the Hopfield network—basically a neural network model inspired by spin glasses. It uses ideas from statistical physics like energy landscapes and mean-field theory, and it ended up laying the groundwork for modern machine learning.

Now, the personal bit:

In the summer of 2022, I was doing an internship at EPFL in Switzerland—and guess what I was working on? The Hopfield model! I was trying to apply it to a problem in high-dimensional combinatorics (graph bi-partitioning, to be exact). So, I was basically using physics, inspired tools to understand a math problem about how to split up large, messy graphs. That same summer, Parisi had just won the Nobel, and I remember feeling like, “Whoa, I’m really in the right place at the right time.”

Looking back, it feels kind of surreal. I got into this stuff out of pure curiosity, and now the models and names I was reading about back then are getting Nobel and Abel Prizes. The crossover between physics, math, and machine learning is more alive than ever—and honestly, it’s such a cool time to be part of it.

It reminds me a bit of Einstein’s Nobel story. You’d think he got it for relativity, right? But nope—he won for explaining the photoelectric effect. Sometimes, it’s not the biggest, flashiest ideas that get recognized first—it’s the unexpected applications that end up changing everything.

I feel like something similar is happening right now with things like random matrix theory, spin glasses, and concentration of measure. They all started in statistical physics, but now they’re turning into seriously powerful tools in areas way beyond their original context.

Like in machine learning—these concepts are now used to understand how algorithms work, how they converge, how randomness behaves in high dimensions, and even how neural networks learn over time.

And that brings us back to Hopfield. His model gave physicists a way to use mean-field theory to approximate complex systems. Fast forward a few decades, and researchers realized: “Hey, this works for neural nets too!” So the Hopfield model became a big deal again—this time in ML—helping simplify complex calculations and giving us new ways to think about learning in high-dimensional spaces.

Just like how Einstein’s photoelectric effect opened the door to quantum theory, Hopfield’s spin-glass inspired work is now influencing how we think about intelligence and learning. Science has this weird way of circling back—and it’s kind of beautiful.
